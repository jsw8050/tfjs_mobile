<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, user-scalable=yes, initial-scale=1, maximum-scale=1">
    <title>Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.1"></script>
</head>

<body>
    <video playsinline autoplay></video>
    <button>Take snapshot</button>
    <canvas></canvas>
    <span></span>
    <script>
        video = document.querySelector('video');
        canvas = document.querySelector('canvas');
        button = document.querySelector('button');
        span = document.querySelector('span');

        canvas.width = 48
        canvas.height = 48

        canvas.style.filter = 'grayscale(1)'; // í‘ë°±...
        video.style.transform = 'scaleX(-1)'; // ì¢Œìš° ë°˜ì „

        span.style.fontSize = '100px'; // ì´ëª¨í‹°ì½˜ ì˜ì—­ í°íŠ¸ ì‚¬ì´ì¦ˆ ì¡°ì ˆ.

        // ë¼ë²¨ , ì´ëª¨í‹°ì½˜ ë§¤í•‘.
        const LABELS = {
            0: 'ğŸ¤¬', // angry
            1: 'ğŸ¤¢', // disgust
            2: 'ğŸ˜±', // fear
            3: 'ğŸ˜„', // happy
            4: 'ğŸ˜¢', // sad
            5: 'ğŸ˜²', // surprise
            6: 'ğŸ˜' // neutral
        }


        //BUTTON EVENT FN
        button.onclick = function() {

            w = video.videoWidth;
            h = video.videoHeight;
            s = Math.min(w, h);
            sx = (w - s) / 2;
            sy = (h - s) / 2;

            canvas.getContext('2d').drawImage(video, sx = sx, sy = sy, swidth = s,
                sheight = s, x = 0, y = 0, width = 48, height = 48);
            //context.drawImage(img, sx, sy, swidth, sheight, x, y, width, height);
            //img: ì´ë¯¸ì§€ ì†ŒìŠ¤, sx: ì´ë¯¸ì§€ì—ì„œ ì˜ë¼ ê°€ì ¸ì˜¬ ì‹œì‘ì ì˜ xì¢Œí‘œ, sy: ì´ë¯¸ì§€ì—ì„œ ì˜ë¼ ê°€ì ¸ì˜¬ ì‹œì‘ì ì˜ yì¢Œí‘œ
            //swidth: ì˜ë¼ ê°€ì ¸ì˜¬ ì´ë¯¸ì§€ì˜ í­ sheight: ì˜ë¼ ê°€ì ¸ì˜¬ ì´ë¯¸ì§€ì˜ ë†’ì´
            //x: ì´ë¯¸ì§€ê°€ ê·¸ë ¤ì§€ëŠ” xì¢Œí‘œ, y: ì´ë¯¸ì§€ê°€ ê·¸ë ¤ì§€ëŠ” yì¢Œí‘œ, width: ì´ë¯¸ì§€ í­, height: ì´ë¯¸ì§€ ë†’ì´

            span.innerHTML = 'âŒ›'; // ê¸°ë‹¤ë¦¬ëŠ” ì´ëª¨í‹°ì½˜ìœ¼ë¡œ...
            predict();
        };


        // obileNet ëª¨ë¸ predict() ë©”ì†Œë“œ
        async function predict() {
            // ëª¨ë¸ import
            const model = await tf.loadLayersModel('./model.json');

            image = tf.browser.fromPixels(canvas); // canvasì— ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
            console.log(image); // ì½˜ì†”ì— ì´ë¯¸ì§€ data ì°ê¸°.

            image = image.toFloat().mean(2).mul(1 / 255.0).reshape([-1, 48, 48, 1]);
            logits = model.predict(image);
            const results = await logits.softmax().data();
            i = results.indexOf(Math.max(...results));

            image.dispose();
            logits.dispose();
            console.log(results);

            span.innerHTML = LABELS[i];
        }

        function faceRecognition() {

        }



        // ë¹„ë””ì˜¤...
        constraints = {
            audio: false,
            video: true
        };

        function handleSuccess(stream) {
            video.srcObject = stream;
        }

        function handleError(error) {
            alert('navigator.MediaDevices.getUserMedia error: ' + error.message + error.name);
        }

        navigator.mediaDevices.getUserMedia(constraints).then(handleSuccess).catch(handleError);
    </script>
</body>

</html>
